{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hekz8LsUrFYF"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout , Activation, Flatten , Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "labels = []\n",
        "docs = []\n",
        "ignore_list = ['?', '!']"
      ],
      "metadata": {
        "id": "ZsGDbvvRrLlW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = open('/content/intents.json').read()\n",
        "intents = json.loads(dataset)"
      ],
      "metadata": {
        "id": "_qz3Owf4rUdV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spQHbjb0tJ-E",
        "outputId": "ac1f4c02-1c93-4dbb-8fb5-cd6586f44142"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        #tokenize each word\n",
        "        word_token = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_token)\n",
        "        #add documents in the corpus\n",
        "        docs.append((word_token, intent['tag']))\n",
        "\n",
        "        # add to our labels list\n",
        "        if intent['tag'] not in labels:\n",
        "            labels.append(intent['tag'])"
      ],
      "metadata": {
        "id": "XYf48HI8sggH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FYOfl4ctd2p",
        "outputId": "d40f5a5e-f893-4752-9f50-899fd894072c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize each word, and sort words by removing duplicates:\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_list]\n",
        "words = sorted(list(set(words)))\n",
        "# sort labels:\n",
        "labels = sorted(list(set(labels)))"
      ],
      "metadata": {
        "id": "he3klSEMtFyV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(labels,open('labels.pkl','wb'))"
      ],
      "metadata": {
        "id": "P15_palAtPtN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our training data:\n",
        "training_data = []\n",
        "# creating an empty array for our output (with size same as length of labels):\n",
        "output = [0]*len(labels)\n",
        "\n",
        "for doc in docs:\n",
        "    bag_of_words = []\n",
        "    pattern_words = doc[0]\n",
        "    #lemmatize pattern words:\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "    for w in words:\n",
        "        if w in pattern_words:\n",
        "            bag_of_words.append(1)\n",
        "        else:\n",
        "            bag_of_words.append(0)\n",
        "\n",
        "    output_row = list(output)\n",
        "    output_row[labels.index(doc[1])] = 1\n",
        "\n",
        "    training_data.append([bag_of_words,output_row])"
      ],
      "metadata": {
        "id": "y8GFMn2btjsu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert training_data to numpy array and shuffle the data:\n",
        "random.shuffle(training_data)\n",
        "training_data = np.array(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0m3yScYtmGy",
        "outputId": "78cfb0cf-5f0d-49d5-d40f-a0284d3377d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-d2a7c1a73fe0>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training_data = np.array(training_data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have to create training list:\n",
        "x_train = list(training_data[:,0])\n",
        "y_train = list(training_data[:,1])"
      ],
      "metadata": {
        "id": "LoaAKfj5tpPi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Model:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(x_train[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(y_train[0]), activation='softmax'))"
      ],
      "metadata": {
        "id": "ekbuJkNxtsln"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxyacCWutvIM",
        "outputId": "5a15f834-9183-440b-81a9-66ee3a9b0b02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               16768     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 30)                1950      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26974 (105.37 KB)\n",
            "Trainable params: 26974 (105.37 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create the SGD optimizer without the decay parameter\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile your model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd_optimizer, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "45_qWjy5t3Hy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "history = model.fit(np.array(x_train), np.array(y_train), epochs=200, batch_size=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beKjVL_qvAPM",
        "outputId": "77c96d15-a6c5-4acf-f354-723eb6b79a93"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 1s 3ms/step - loss: 3.4167 - accuracy: 0.0354\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.3213 - accuracy: 0.0885\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.3098 - accuracy: 0.1327\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.2586 - accuracy: 0.0973\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.1624 - accuracy: 0.1416\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 3.0764 - accuracy: 0.1947\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 3.0164 - accuracy: 0.1770\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.9159 - accuracy: 0.2478\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.8559 - accuracy: 0.2478\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.7487 - accuracy: 0.2389\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.6585 - accuracy: 0.3274\n",
            "Epoch 12/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.5669 - accuracy: 0.3009\n",
            "Epoch 13/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.4548 - accuracy: 0.3628\n",
            "Epoch 14/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.3178 - accuracy: 0.3717\n",
            "Epoch 15/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 2.3173 - accuracy: 0.3628\n",
            "Epoch 16/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2.0736 - accuracy: 0.4690\n",
            "Epoch 17/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9967 - accuracy: 0.3982\n",
            "Epoch 18/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.9741 - accuracy: 0.4071\n",
            "Epoch 19/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8873 - accuracy: 0.5044\n",
            "Epoch 20/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.8473 - accuracy: 0.4779\n",
            "Epoch 21/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.8307 - accuracy: 0.4867\n",
            "Epoch 22/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6532 - accuracy: 0.5664\n",
            "Epoch 23/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.6014 - accuracy: 0.5575\n",
            "Epoch 24/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.4799 - accuracy: 0.5575\n",
            "Epoch 25/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1.4319 - accuracy: 0.5841\n",
            "Epoch 26/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3235 - accuracy: 0.6195\n",
            "Epoch 27/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3061 - accuracy: 0.6106\n",
            "Epoch 28/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.3670 - accuracy: 0.6195\n",
            "Epoch 29/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.2305 - accuracy: 0.5752\n",
            "Epoch 30/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.2369 - accuracy: 0.6814\n",
            "Epoch 31/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.1020 - accuracy: 0.6903\n",
            "Epoch 32/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 1.1691 - accuracy: 0.6637\n",
            "Epoch 33/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9519 - accuracy: 0.6903\n",
            "Epoch 34/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9037 - accuracy: 0.7699\n",
            "Epoch 35/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9031 - accuracy: 0.7168\n",
            "Epoch 36/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.9212 - accuracy: 0.7434\n",
            "Epoch 37/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7914 - accuracy: 0.7788\n",
            "Epoch 38/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7320 - accuracy: 0.7965\n",
            "Epoch 39/200\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8839 - accuracy: 0.7168\n",
            "Epoch 40/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.7421 - accuracy: 0.7965\n",
            "Epoch 41/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.8696 - accuracy: 0.7345\n",
            "Epoch 42/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.7876\n",
            "Epoch 43/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7598 - accuracy: 0.7611\n",
            "Epoch 44/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.8142\n",
            "Epoch 45/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7246 - accuracy: 0.7611\n",
            "Epoch 46/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.5115 - accuracy: 0.8496\n",
            "Epoch 47/200\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5809 - accuracy: 0.8142\n",
            "Epoch 48/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7163 - accuracy: 0.7699\n",
            "Epoch 49/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7454 - accuracy: 0.7965\n",
            "Epoch 50/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.7132 - accuracy: 0.7965\n",
            "Epoch 51/200\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.5594 - accuracy: 0.8496\n",
            "Epoch 52/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.8673\n",
            "Epoch 53/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.6523 - accuracy: 0.7699\n",
            "Epoch 54/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.8761\n",
            "Epoch 55/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.4666 - accuracy: 0.9115\n",
            "Epoch 56/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.5646 - accuracy: 0.8319\n",
            "Epoch 57/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.4867 - accuracy: 0.8584\n",
            "Epoch 58/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.4755 - accuracy: 0.8584\n",
            "Epoch 59/200\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.4124 - accuracy: 0.9027\n",
            "Epoch 60/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4597 - accuracy: 0.8938\n",
            "Epoch 61/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.5415 - accuracy: 0.8230\n",
            "Epoch 62/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.4081 - accuracy: 0.8761\n",
            "Epoch 63/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.3988 - accuracy: 0.8673\n",
            "Epoch 64/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.3348 - accuracy: 0.9204\n",
            "Epoch 65/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.3671 - accuracy: 0.9204\n",
            "Epoch 66/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.8584\n",
            "Epoch 67/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.8584\n",
            "Epoch 68/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8761\n",
            "Epoch 69/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.9204\n",
            "Epoch 70/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8584\n",
            "Epoch 71/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.9027\n",
            "Epoch 72/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.9469\n",
            "Epoch 73/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8850\n",
            "Epoch 74/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8850\n",
            "Epoch 75/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.9558\n",
            "Epoch 76/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.9027\n",
            "Epoch 77/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.9204\n",
            "Epoch 78/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8761\n",
            "Epoch 79/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.9469\n",
            "Epoch 80/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.9204\n",
            "Epoch 81/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8938\n",
            "Epoch 82/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3844 - accuracy: 0.8850\n",
            "Epoch 83/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2374 - accuracy: 0.9381\n",
            "Epoch 84/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1981 - accuracy: 0.9292\n",
            "Epoch 85/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8584\n",
            "Epoch 86/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3731 - accuracy: 0.9027\n",
            "Epoch 87/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.9115\n",
            "Epoch 88/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.9204\n",
            "Epoch 89/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.9115\n",
            "Epoch 90/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.9115\n",
            "Epoch 91/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9646\n",
            "Epoch 92/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.8938\n",
            "Epoch 93/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.9292\n",
            "Epoch 94/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.9558\n",
            "Epoch 95/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8761\n",
            "Epoch 96/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9381\n",
            "Epoch 97/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9558\n",
            "Epoch 98/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.9292\n",
            "Epoch 99/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.9115\n",
            "Epoch 100/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9204\n",
            "Epoch 101/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9292\n",
            "Epoch 102/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.9292\n",
            "Epoch 103/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9115\n",
            "Epoch 104/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.9027\n",
            "Epoch 105/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3045 - accuracy: 0.9115\n",
            "Epoch 106/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.9292\n",
            "Epoch 107/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.9115\n",
            "Epoch 108/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.9292\n",
            "Epoch 109/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2897 - accuracy: 0.9115\n",
            "Epoch 110/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9292\n",
            "Epoch 111/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.8938\n",
            "Epoch 112/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9558\n",
            "Epoch 113/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.9204\n",
            "Epoch 114/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9292\n",
            "Epoch 115/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2132 - accuracy: 0.9469\n",
            "Epoch 116/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9469\n",
            "Epoch 117/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.9646\n",
            "Epoch 118/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9381\n",
            "Epoch 119/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9558\n",
            "Epoch 120/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9027\n",
            "Epoch 121/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.9381\n",
            "Epoch 122/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9292\n",
            "Epoch 123/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9469\n",
            "Epoch 124/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 0.9646\n",
            "Epoch 125/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9115\n",
            "Epoch 126/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9381\n",
            "Epoch 127/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9735\n",
            "Epoch 128/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2583 - accuracy: 0.8938\n",
            "Epoch 129/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.9115\n",
            "Epoch 130/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.9027\n",
            "Epoch 131/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9646\n",
            "Epoch 132/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1357 - accuracy: 0.9292\n",
            "Epoch 133/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9204\n",
            "Epoch 134/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1696 - accuracy: 0.9381\n",
            "Epoch 135/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9469\n",
            "Epoch 136/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1746 - accuracy: 0.9469\n",
            "Epoch 137/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9558\n",
            "Epoch 138/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2177 - accuracy: 0.9115\n",
            "Epoch 139/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1633 - accuracy: 0.9558\n",
            "Epoch 140/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1524 - accuracy: 0.9558\n",
            "Epoch 141/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9558\n",
            "Epoch 142/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9381\n",
            "Epoch 143/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.9292\n",
            "Epoch 144/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1850 - accuracy: 0.9204\n",
            "Epoch 145/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9292\n",
            "Epoch 146/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.9558\n",
            "Epoch 147/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9558\n",
            "Epoch 148/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1571 - accuracy: 0.9469\n",
            "Epoch 149/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9469\n",
            "Epoch 150/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1245 - accuracy: 0.9646\n",
            "Epoch 151/200\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9469\n",
            "Epoch 152/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.9558\n",
            "Epoch 153/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9558\n",
            "Epoch 154/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9292\n",
            "Epoch 155/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1146 - accuracy: 0.9646\n",
            "Epoch 156/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9292\n",
            "Epoch 157/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9558\n",
            "Epoch 158/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.9646\n",
            "Epoch 159/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9558\n",
            "Epoch 160/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9027\n",
            "Epoch 161/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9646\n",
            "Epoch 162/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.9558\n",
            "Epoch 163/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9646\n",
            "Epoch 164/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9646\n",
            "Epoch 165/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9469\n",
            "Epoch 166/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9735\n",
            "Epoch 167/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.9381\n",
            "Epoch 168/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9115\n",
            "Epoch 169/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9646\n",
            "Epoch 170/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1209 - accuracy: 0.9646\n",
            "Epoch 171/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9558\n",
            "Epoch 172/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.9558\n",
            "Epoch 173/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 0.9115\n",
            "Epoch 174/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9381\n",
            "Epoch 175/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.9735\n",
            "Epoch 176/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.9204\n",
            "Epoch 177/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9646\n",
            "Epoch 178/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9469\n",
            "Epoch 179/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.9912\n",
            "Epoch 180/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.9381\n",
            "Epoch 181/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1015 - accuracy: 0.9646\n",
            "Epoch 182/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9292\n",
            "Epoch 183/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9823\n",
            "Epoch 184/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9646\n",
            "Epoch 185/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1614 - accuracy: 0.9558\n",
            "Epoch 186/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0893 - accuracy: 0.9646\n",
            "Epoch 187/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.9204\n",
            "Epoch 188/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2311 - accuracy: 0.9115\n",
            "Epoch 189/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9735\n",
            "Epoch 190/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1882 - accuracy: 0.9469\n",
            "Epoch 191/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1286 - accuracy: 0.9381\n",
            "Epoch 192/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9558\n",
            "Epoch 193/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9558\n",
            "Epoch 194/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9558\n",
            "Epoch 195/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1519 - accuracy: 0.9558\n",
            "Epoch 196/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9735\n",
            "Epoch 197/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1490 - accuracy: 0.9381\n",
            "Epoch 198/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.9823\n",
            "Epoch 199/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.9646\n",
            "Epoch 200/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('ChatBotModel', history)"
      ],
      "metadata": {
        "id": "J6xrhV1_vEm2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('ChatBotModel')\n",
        "\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "labels = pickle.load(open('labels.pkl','rb'))"
      ],
      "metadata": {
        "id": "zDzGcFFXvR5F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bank_of_words(s,words, show_details=True):\n",
        "    bag_of_words = [0 for _ in range(len(words))]\n",
        "    sent_words = nltk.word_tokenize(s)\n",
        "    sent_words = [lemmatizer.lemmatize(word.lower()) for word in sent_words]\n",
        "    for sent in sent_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == sent:\n",
        "                bag_of_words[i] = 1\n",
        "    return np.array(bag_of_words)\n",
        "\n",
        "def predict_label(s, model):\n",
        "    # filtering out predictions\n",
        "    pred = bank_of_words(s, words,show_details=False)\n",
        "    response = model.predict(np.array([pred]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    final_results = [[i,r] for i,r in enumerate(response) if r>ERROR_THRESHOLD]\n",
        "    final_results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in final_results:\n",
        "        return_list.append({\"intent\": labels[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list"
      ],
      "metadata": {
        "id": "P3jQKHqtvYNS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Response(ints, intents_json):\n",
        "    tags = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tags):\n",
        "            response = random.choice(i['responses'])\n",
        "            break\n",
        "    return response\n",
        "\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_label(msg, model)\n",
        "    response = Response(ints, intents)\n",
        "    return response"
      ],
      "metadata": {
        "id": "htrZMwMGveHB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"Start chat with ChatBot of Deepu\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        if inp.lower() == 'quit':\n",
        "            break\n",
        "        response = chatbot_response(inp)\n",
        "        print(\"\\n BOT: \" + response + '\\n\\n')\n",
        "\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0exSpkKMviHj",
        "outputId": "fd1eb458-9ddb-4ad4-c6cb-c7e9191246af"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start chat with ChatBot of Deepu\n",
            "You: Hii\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "\n",
            " BOT: Redirecting to Google...\n",
            "\n",
            "\n",
            "You: Hi\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "\n",
            " BOT: Hello\n",
            "\n",
            "\n",
            "You: jfakdfnidnhf\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "\n",
            " BOT: Redirecting to Google...\n",
            "\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PbzZVSWDvkaX"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}